{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OneR algorithm is quite simple but can be quite effective, showing the power of using even basic statistics in many applications.\n",
    "The algorithm is:\n",
    "\n",
    "* For each variable\n",
    "    * For each value of the variable\n",
    "        * The prediction based on this variable goes the most frequent class\n",
    "        * Compute the error of this prediction\n",
    "    * Sum the prediction errors for all values of the variable\n",
    "* Use the variable with the lowest error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function array in numpy:\n",
      "\n",
      "numpy.array = array(...)\n",
      "    array(object, dtype=None, copy=True, order=None, subok=False, ndmin=0)\n",
      "    \n",
      "    Create an array.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    object : array_like\n",
      "        An array, any object exposing the array interface, an\n",
      "        object whose __array__ method returns an array, or any\n",
      "        (nested) sequence.\n",
      "    dtype : data-type, optional\n",
      "        The desired data-type for the array.  If not given, then\n",
      "        the type will be determined as the minimum type required\n",
      "        to hold the objects in the sequence.  This argument can only\n",
      "        be used to 'upcast' the array.  For downcasting, use the\n",
      "        .astype(t) method.\n",
      "    copy : bool, optional\n",
      "        If true (default), then the object is copied.  Otherwise, a copy\n",
      "        will only be made if __array__ returns a copy, if obj is a\n",
      "        nested sequence, or if a copy is needed to satisfy any of the other\n",
      "        requirements (`dtype`, `order`, etc.).\n",
      "    order : {'C', 'F', 'A'}, optional\n",
      "        Specify the order of the array.  If order is 'C', then the array\n",
      "        will be in C-contiguous order (last-index varies the fastest).\n",
      "        If order is 'F', then the returned array will be in\n",
      "        Fortran-contiguous order (first-index varies the fastest).\n",
      "        If order is 'A' (default), then the returned array may be\n",
      "        in any order (either C-, Fortran-contiguous, or even discontiguous),\n",
      "        unless a copy is required, in which case it will be C-contiguous.\n",
      "    subok : bool, optional\n",
      "        If True, then sub-classes will be passed-through, otherwise\n",
      "        the returned array will be forced to be a base-class array (default).\n",
      "    ndmin : int, optional\n",
      "        Specifies the minimum number of dimensions that the resulting\n",
      "        array should have.  Ones will be pre-pended to the shape as\n",
      "        needed to meet this requirement.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray\n",
      "        An array object satisfying the specified requirements.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    empty, empty_like, zeros, zeros_like, ones, ones_like, fill\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.array([1, 2, 3])\n",
      "    array([1, 2, 3])\n",
      "    \n",
      "    Upcasting:\n",
      "    \n",
      "    >>> np.array([1, 2, 3.0])\n",
      "    array([ 1.,  2.,  3.])\n",
      "    \n",
      "    More than one dimension:\n",
      "    \n",
      "    >>> np.array([[1, 2], [3, 4]])\n",
      "    array([[1, 2],\n",
      "           [3, 4]])\n",
      "    \n",
      "    Minimum dimensions 2:\n",
      "    \n",
      "    >>> np.array([1, 2, 3], ndmin=2)\n",
      "    array([[1, 2, 3]])\n",
      "    \n",
      "    Type provided:\n",
      "    \n",
      "    >>> np.array([1, 2, 3], dtype=complex)\n",
      "    array([ 1.+0.j,  2.+0.j,  3.+0.j])\n",
      "    \n",
      "    Data-type consisting of more than one element:\n",
      "    \n",
      "    >>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n",
      "    >>> x['a']\n",
      "    array([1, 3])\n",
      "    \n",
      "    Creating an array from sub-classes:\n",
      "    \n",
      "    >>> np.array(np.mat('1 2; 3 4'))\n",
      "    array([[1, 2],\n",
      "           [3, 4]])\n",
      "    \n",
      "    >>> np.array(np.mat('1 2; 3 4'), subok=True)\n",
      "    matrix([[1, 2],\n",
      "            [3, 4]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('numpy.array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 5.4  3.9  1.7  0.4]\n",
      " [ 4.6  3.4  1.4  0.3]\n",
      " [ 5.   3.4  1.5  0.2]\n",
      " [ 4.4  2.9  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.4  3.7  1.5  0.2]\n",
      " [ 4.8  3.4  1.6  0.2]\n",
      " [ 4.8  3.   1.4  0.1]\n",
      " [ 4.3  3.   1.1  0.1]\n",
      " [ 5.8  4.   1.2  0.2]\n",
      " [ 5.7  4.4  1.5  0.4]\n",
      " [ 5.4  3.9  1.3  0.4]\n",
      " [ 5.1  3.5  1.4  0.3]\n",
      " [ 5.7  3.8  1.7  0.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 5.4  3.4  1.7  0.2]\n",
      " [ 5.1  3.7  1.5  0.4]\n",
      " [ 4.6  3.6  1.   0.2]\n",
      " [ 5.1  3.3  1.7  0.5]\n",
      " [ 4.8  3.4  1.9  0.2]\n",
      " [ 5.   3.   1.6  0.2]\n",
      " [ 5.   3.4  1.6  0.4]\n",
      " [ 5.2  3.5  1.5  0.2]\n",
      " [ 5.2  3.4  1.4  0.2]\n",
      " [ 4.7  3.2  1.6  0.2]\n",
      " [ 4.8  3.1  1.6  0.2]\n",
      " [ 5.4  3.4  1.5  0.4]\n",
      " [ 5.2  4.1  1.5  0.1]\n",
      " [ 5.5  4.2  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.   3.2  1.2  0.2]\n",
      " [ 5.5  3.5  1.3  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 4.4  3.   1.3  0.2]\n",
      " [ 5.1  3.4  1.5  0.2]\n",
      " [ 5.   3.5  1.3  0.3]\n",
      " [ 4.5  2.3  1.3  0.3]\n",
      " [ 4.4  3.2  1.3  0.2]\n",
      " [ 5.   3.5  1.6  0.6]\n",
      " [ 5.1  3.8  1.9  0.4]\n",
      " [ 4.8  3.   1.4  0.3]\n",
      " [ 5.1  3.8  1.6  0.2]\n",
      " [ 4.6  3.2  1.4  0.2]\n",
      " [ 5.3  3.7  1.5  0.2]\n",
      " [ 5.   3.3  1.4  0.2]\n",
      " [ 7.   3.2  4.7  1.4]\n",
      " [ 6.4  3.2  4.5  1.5]\n",
      " [ 6.9  3.1  4.9  1.5]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 6.5  2.8  4.6  1.5]\n",
      " [ 5.7  2.8  4.5  1.3]\n",
      " [ 6.3  3.3  4.7  1.6]\n",
      " [ 4.9  2.4  3.3  1. ]\n",
      " [ 6.6  2.9  4.6  1.3]\n",
      " [ 5.2  2.7  3.9  1.4]\n",
      " [ 5.   2.   3.5  1. ]\n",
      " [ 5.9  3.   4.2  1.5]\n",
      " [ 6.   2.2  4.   1. ]\n",
      " [ 6.1  2.9  4.7  1.4]\n",
      " [ 5.6  2.9  3.6  1.3]\n",
      " [ 6.7  3.1  4.4  1.4]\n",
      " [ 5.6  3.   4.5  1.5]\n",
      " [ 5.8  2.7  4.1  1. ]\n",
      " [ 6.2  2.2  4.5  1.5]\n",
      " [ 5.6  2.5  3.9  1.1]\n",
      " [ 5.9  3.2  4.8  1.8]\n",
      " [ 6.1  2.8  4.   1.3]\n",
      " [ 6.3  2.5  4.9  1.5]\n",
      " [ 6.1  2.8  4.7  1.2]\n",
      " [ 6.4  2.9  4.3  1.3]\n",
      " [ 6.6  3.   4.4  1.4]\n",
      " [ 6.8  2.8  4.8  1.4]\n",
      " [ 6.7  3.   5.   1.7]\n",
      " [ 6.   2.9  4.5  1.5]\n",
      " [ 5.7  2.6  3.5  1. ]\n",
      " [ 5.5  2.4  3.8  1.1]\n",
      " [ 5.5  2.4  3.7  1. ]\n",
      " [ 5.8  2.7  3.9  1.2]\n",
      " [ 6.   2.7  5.1  1.6]\n",
      " [ 5.4  3.   4.5  1.5]\n",
      " [ 6.   3.4  4.5  1.6]\n",
      " [ 6.7  3.1  4.7  1.5]\n",
      " [ 6.3  2.3  4.4  1.3]\n",
      " [ 5.6  3.   4.1  1.3]\n",
      " [ 5.5  2.5  4.   1.3]\n",
      " [ 5.5  2.6  4.4  1.2]\n",
      " [ 6.1  3.   4.6  1.4]\n",
      " [ 5.8  2.6  4.   1.2]\n",
      " [ 5.   2.3  3.3  1. ]\n",
      " [ 5.6  2.7  4.2  1.3]\n",
      " [ 5.7  3.   4.2  1.2]\n",
      " [ 5.7  2.9  4.2  1.3]\n",
      " [ 6.2  2.9  4.3  1.3]\n",
      " [ 5.1  2.5  3.   1.1]\n",
      " [ 5.7  2.8  4.1  1.3]\n",
      " [ 6.3  3.3  6.   2.5]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 7.1  3.   5.9  2.1]\n",
      " [ 6.3  2.9  5.6  1.8]\n",
      " [ 6.5  3.   5.8  2.2]\n",
      " [ 7.6  3.   6.6  2.1]\n",
      " [ 4.9  2.5  4.5  1.7]\n",
      " [ 7.3  2.9  6.3  1.8]\n",
      " [ 6.7  2.5  5.8  1.8]\n",
      " [ 7.2  3.6  6.1  2.5]\n",
      " [ 6.5  3.2  5.1  2. ]\n",
      " [ 6.4  2.7  5.3  1.9]\n",
      " [ 6.8  3.   5.5  2.1]\n",
      " [ 5.7  2.5  5.   2. ]\n",
      " [ 5.8  2.8  5.1  2.4]\n",
      " [ 6.4  3.2  5.3  2.3]\n",
      " [ 6.5  3.   5.5  1.8]\n",
      " [ 7.7  3.8  6.7  2.2]\n",
      " [ 7.7  2.6  6.9  2.3]\n",
      " [ 6.   2.2  5.   1.5]\n",
      " [ 6.9  3.2  5.7  2.3]\n",
      " [ 5.6  2.8  4.9  2. ]\n",
      " [ 7.7  2.8  6.7  2. ]\n",
      " [ 6.3  2.7  4.9  1.8]\n",
      " [ 6.7  3.3  5.7  2.1]\n",
      " [ 7.2  3.2  6.   1.8]\n",
      " [ 6.2  2.8  4.8  1.8]\n",
      " [ 6.1  3.   4.9  1.8]\n",
      " [ 6.4  2.8  5.6  2.1]\n",
      " [ 7.2  3.   5.8  1.6]\n",
      " [ 7.4  2.8  6.1  1.9]\n",
      " [ 7.9  3.8  6.4  2. ]\n",
      " [ 6.4  2.8  5.6  2.2]\n",
      " [ 6.3  2.8  5.1  1.5]\n",
      " [ 6.1  2.6  5.6  1.4]\n",
      " [ 7.7  3.   6.1  2.3]\n",
      " [ 6.3  3.4  5.6  2.4]\n",
      " [ 6.4  3.1  5.5  1.8]\n",
      " [ 6.   3.   4.8  1.8]\n",
      " [ 6.9  3.1  5.4  2.1]\n",
      " [ 6.7  3.1  5.6  2.4]\n",
      " [ 6.9  3.1  5.1  2.3]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 6.8  3.2  5.9  2.3]\n",
      " [ 6.7  3.3  5.7  2.5]\n",
      " [ 6.7  3.   5.2  2.3]\n",
      " [ 6.3  2.5  5.   1.9]\n",
      " [ 6.5  3.   5.2  2. ]\n",
      " [ 6.2  3.4  5.4  2.3]\n",
      " [ 5.9  3.   5.1  1.8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.55 ,  2.375,  2.35 ,  2.35 ,  2.55 ,  2.85 ,  2.425,  2.525,\n",
       "        2.225,  2.4  ,  2.7  ,  2.5  ,  2.325,  2.125,  2.8  ,  3.   ,\n",
       "        2.75 ,  2.575,  2.875,  2.675,  2.675,  2.675,  2.35 ,  2.65 ,\n",
       "        2.575,  2.45 ,  2.6  ,  2.6  ,  2.55 ,  2.425,  2.425,  2.675,\n",
       "        2.725,  2.825,  2.4  ,  2.4  ,  2.625,  2.4  ,  2.225,  2.55 ,\n",
       "        2.525,  2.1  ,  2.275,  2.675,  2.8  ,  2.375,  2.675,  2.35 ,\n",
       "        2.675,  2.475,  4.075,  3.9  ,  4.1  ,  3.275,  3.85 ,  3.575,\n",
       "        3.975,  2.9  ,  3.85 ,  3.3  ,  2.875,  3.65 ,  3.3  ,  3.775,\n",
       "        3.35 ,  3.9  ,  3.65 ,  3.4  ,  3.6  ,  3.275,  3.925,  3.55 ,\n",
       "        3.8  ,  3.7  ,  3.725,  3.85 ,  3.95 ,  4.1  ,  3.725,  3.2  ,\n",
       "        3.2  ,  3.15 ,  3.4  ,  3.85 ,  3.6  ,  3.875,  4.   ,  3.575,\n",
       "        3.5  ,  3.325,  3.425,  3.775,  3.4  ,  2.9  ,  3.45 ,  3.525,\n",
       "        3.525,  3.675,  2.925,  3.475,  4.525,  3.875,  4.525,  4.15 ,\n",
       "        4.375,  4.825,  3.4  ,  4.575,  4.2  ,  4.85 ,  4.2  ,  4.075,\n",
       "        4.35 ,  3.8  ,  4.025,  4.3  ,  4.2  ,  5.1  ,  4.875,  3.675,\n",
       "        4.525,  3.825,  4.8  ,  3.925,  4.45 ,  4.55 ,  3.9  ,  3.95 ,\n",
       "        4.225,  4.4  ,  4.55 ,  5.025,  4.25 ,  3.925,  3.925,  4.775,\n",
       "        4.425,  4.2  ,  3.9  ,  4.375,  4.45 ,  4.35 ,  3.875,  4.55 ,\n",
       "        4.55 ,  4.3  ,  3.925,  4.175,  4.325,  3.95 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load our dataset\n",
    "from sklearn.datasets import load_iris\n",
    "#X, y = np.loadtxt(\"X_classification.txt\"), np.loadtxt(\"y_classification.txt\")\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "# print(dataset.DESCR)\n",
    "print(X)\n",
    "n_samples, n_features = X.shape\n",
    "attribute_means = X.mean(axis=1)\n",
    "attribute_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our attributes are continuous, while we want categorical features to use OneR. We will perform a *preprocessing* step called discretisation. At this stage, we will perform a simple procedure: compute the mean and determine whether a value is above or below the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the mean for each attribute\n",
    "attribute_means = X.mean(axis=0)\n",
    "assert attribute_means.shape == (n_features,)\n",
    "X_d = np.array(X >= attribute_means, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are (112,) training samples\n",
      "There are (38,) testing samples\n"
     ]
    }
   ],
   "source": [
    "# Now, we split into a training and test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Set the random state to the same number to get the same results as in the book\n",
    "random_state = 14\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d, y, random_state=random_state)\n",
    "print(\"There are {} training samples\".format(y_train.shape))\n",
    "print(\"There are {} testing samples\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def train(X, y_true, feature):\n",
    "    \"\"\"Computes the predictors and error for a given feature using the OneR algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array [n_samples, n_features]\n",
    "        The two dimensional array that holds the dataset. Each row is a sample, each column\n",
    "        is a feature.\n",
    "    \n",
    "    y_true: array [n_samples,]\n",
    "        The one dimensional array that holds the class values. Corresponds to X, such that\n",
    "        y_true[i] is the class value for sample X[i].\n",
    "    \n",
    "    feature: int\n",
    "        An integer corresponding to the index of the variable we wish to test.\n",
    "        0 <= variable < n_features\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    predictors: dictionary of tuples: (value, prediction)\n",
    "        For each item in the array, if the variable has a given value, make the given prediction.\n",
    "    \n",
    "    error: float\n",
    "        The ratio of training data that this rule incorrectly predicts.\n",
    "    \"\"\"\n",
    "    # Check that variable is a valid number\n",
    "    n_samples, n_features = X.shape\n",
    "    assert 0 <= feature < n_features\n",
    "    # Get all of the unique values that this variable has\n",
    "    values = set(X[:,feature])\n",
    "    # Stores the predictors array that is returned\n",
    "    predictors = dict()\n",
    "    errors = []\n",
    "    for current_value in values:\n",
    "        most_frequent_class, error = train_feature_value(X, y_true, feature, current_value)\n",
    "        predictors[current_value] = most_frequent_class\n",
    "        errors.append(error)\n",
    "    # Compute the total error of using this feature to classify on\n",
    "    total_error = sum(errors)\n",
    "    return predictors, total_error\n",
    "\n",
    "# Compute what our predictors say each sample is based on its value\n",
    "#y_predicted = np.array([predictors[sample[feature]] for sample in X])\n",
    "    \n",
    "\n",
    "def train_feature_value(X, y_true, feature, value):\n",
    "    # Create a simple dictionary to count how frequency they give certain predictions\n",
    "    class_counts = defaultdict(int)\n",
    "    # Iterate through each sample and count the frequency of each class/value pair\n",
    "    for sample, y in zip(X, y_true):\n",
    "        if sample[feature] == value:\n",
    "            class_counts[y] += 1\n",
    "    # Now get the best one by sorting (highest first) and choosing the first item\n",
    "    sorted_class_counts = sorted(class_counts.items(), key=itemgetter(1), reverse=True)\n",
    "    most_frequent_class = sorted_class_counts[0][0]\n",
    "    # The error is the number of samples that do not classify as the most frequent class\n",
    "    # *and* have the feature value.\n",
    "    n_samples = X.shape[1]\n",
    "    error = sum([class_count for class_value, class_count in class_counts.items()\n",
    "                 if class_value != most_frequent_class])\n",
    "    return most_frequent_class, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is based on variable 2 and has error 37.00\n",
      "{'variable': 2, 'predictor': {0: 0, 1: 2}}\n",
      "{0: ({0: 0, 1: 2}, 41), 1: ({0: 1, 1: 0}, 58), 2: ({0: 0, 1: 2}, 37), 3: ({0: 0, 1: 2}, 37)}\n"
     ]
    }
   ],
   "source": [
    "# Compute all of the predictors\n",
    "all_predictors = {variable: train(X_train, y_train, variable) for variable in range(X_train.shape[1])}\n",
    "errors = {variable: error for variable, (mapping, error) in all_predictors.items()}\n",
    "# Now choose the best and save that as \"model\"\n",
    "# Sort by error\n",
    "best_variable, best_error = sorted(errors.items(), key=itemgetter(1))[0]\n",
    "print(\"The best model is based on variable {0} and has error {1:.2f}\".format(best_variable, best_error))\n",
    "\n",
    "# Choose the bset model\n",
    "model = {'variable': best_variable,\n",
    "         'predictor': all_predictors[best_variable][0]}\n",
    "print(model)\n",
    "print(all_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(X_test, model):\n",
    "    variable = model['variable']\n",
    "    predictor = model['predictor']\n",
    "    y_predicted = np.array([predictor[int(sample[variable])] for sample in X_test])\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 2 2 2 0 2 0 2 2 0 2 2 0 2 0 2 2 2 0 0 0 2 0 2 0 2 2 0 0 0 2 0 2 0 2\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = predict(X_test, model)\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 65.8%\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy by taking the mean of the amounts that y_predicted is equal to y_test\n",
    "accuracy = np.mean(y_predicted == y_test) * 100\n",
    "print(\"The test accuracy is {:.1f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_test\n",
    "y_predicted\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        17\n",
      "          1       0.00      0.00      0.00        13\n",
      "          2       0.40      1.00      0.57         8\n",
      "\n",
      "avg / total       0.51      0.66      0.55        38\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\frequently_used\\anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
